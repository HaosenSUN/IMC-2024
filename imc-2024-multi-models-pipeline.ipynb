{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe22984c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:22:57.800175Z",
     "iopub.status.busy": "2024-06-11T09:22:57.799836Z",
     "iopub.status.idle": "2024-06-11T09:22:57.804809Z",
     "shell.execute_reply": "2024-06-11T09:22:57.804063Z"
    },
    "papermill": {
     "duration": 0.013134,
     "end_time": "2024-06-11T09:22:57.806740",
     "exception": false,
     "start_time": "2024-06-11T09:22:57.793606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型与数据集链接\n",
    "#https://www.kaggle.com/competitions/image-matching-challenge-2024\n",
    "#https://www.kaggle.com/datasets/maxchen303/hardnet8v2\n",
    "#https://www.kaggle.com/datasets/oldufo/imc2024-packages-lightglue-rerun-kornia\n",
    "#https://www.kaggle.com/datasets/oldufo/kornia-local-feature-weights\n",
    "#https://www.kaggle.com/datasets/losveria/super-glue-pretrained-network\n",
    "#https://www.kaggle.com/models/oldufo/aliked/PyTorch/aliked-n16/1\n",
    "#https://www.kaggle.com/models/timm/tf-efficientnet/PyTorch/tf-efficientnet-b7/1\n",
    "#https://www.kaggle.com/models/timm/tf-efficientnet/PyTorch/tf-efficientnet-b6/1\n",
    "#https://www.kaggle.com/code/motono0223/pytorch-lightglue-models\n",
    "#https://www.kaggle.com/code/motono0223/dkm-dependencies\n",
    "#https://www.kaggle.com/code/motono0223/dependencies-imc\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4d267",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-11T09:22:57.817133Z",
     "iopub.status.busy": "2024-06-11T09:22:57.816500Z",
     "iopub.status.idle": "2024-06-11T09:24:17.749553Z",
     "shell.execute_reply": "2024-06-11T09:24:17.748535Z"
    },
    "papermill": {
     "duration": 79.940678,
     "end_time": "2024-06-11T09:24:17.752057",
     "exception": false,
     "start_time": "2024-06-11T09:22:57.811379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: pycolmap\r\n",
      "Successfully installed pycolmap-0.4.0\r\n",
      "Processing /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: safetensors\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.4.3\r\n",
      "    Uninstalling safetensors-0.4.3:\r\n",
      "      Successfully uninstalled safetensors-0.4.3\r\n",
      "Successfully installed safetensors-0.4.1\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Installing collected packages: lightglue\r\n",
      "Successfully installed lightglue-0.0\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
    "!python -m pip install  --no-deps /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb81d",
   "metadata": {
    "papermill": {
     "duration": 0.00508,
     "end_time": "2024-06-11T09:24:17.762872",
     "exception": false,
     "start_time": "2024-06-11T09:24:17.757792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9c1edc",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:17.774623Z",
     "iopub.status.busy": "2024-06-11T09:24:17.774308Z",
     "iopub.status.idle": "2024-06-11T09:24:17.779986Z",
     "shell.execute_reply": "2024-06-11T09:24:17.779304Z"
    },
    "papermill": {
     "duration": 0.013912,
     "end_time": "2024-06-11T09:24:17.781868",
     "exception": false,
     "start_time": "2024-06-11T09:24:17.767956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b858357",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:17.793360Z",
     "iopub.status.busy": "2024-06-11T09:24:17.792915Z",
     "iopub.status.idle": "2024-06-11T09:24:27.394365Z",
     "shell.execute_reply": "2024-06-11T09:24:27.393589Z"
    },
    "papermill": {
     "duration": 9.609665,
     "end_time": "2024-06-11T09:24:27.396606",
     "exception": false,
     "start_time": "2024-06-11T09:24:17.786941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# LoFTR\n",
    "from kornia.feature import LoFTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fe6951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:27.408886Z",
     "iopub.status.busy": "2024-06-11T09:24:27.408581Z",
     "iopub.status.idle": "2024-06-11T09:24:27.413556Z",
     "shell.execute_reply": "2024-06-11T09:24:27.412526Z"
    },
    "papermill": {
     "duration": 0.013149,
     "end_time": "2024-06-11T09:24:27.415536",
     "exception": false,
     "start_time": "2024-06-11T09:24:27.402387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.7.2\n",
      "Pycolmap version 0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416dd0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:27.428545Z",
     "iopub.status.busy": "2024-06-11T09:24:27.428302Z",
     "iopub.status.idle": "2024-06-11T09:37:13.205630Z",
     "shell.execute_reply": "2024-06-11T09:37:13.204563Z"
    },
    "papermill": {
     "duration": 765.787066,
     "end_time": "2024-06-11T09:37:13.207965",
     "exception": false,
     "start_time": "2024-06-11T09:24:27.420899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction done in  348.6867 sec\n",
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 22183\n",
      "\tnum_observations = 108426\n",
      "\tmean_track_length = 4.8878\n",
      "\tmean_observations_per_image = 2853.32\n",
      "\tmean_reprojection_error = 0.891438\n",
      "list_num_images = [38]\n",
      "Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 22183\n",
      "\tnum_observations = 108426\n",
      "\tmean_track_length = 4.8878\n",
      "\tmean_observations_per_image = 2853.32\n",
      "\tmean_reprojection_error = 0.891438\n",
      "Registered: church / church -> 38 images\n",
      "Total: church / church -> 41 images\n",
      "{'rotation_detection': [3.814697265625e-06], 'shortlisting': [0.00018930435180664062], 'feature_detection': [], 'feature_matching': [165.66004538536072], 'RANSAC': [7.138962507247925], 'Reconstruction': [348.6867005825043]}\n",
      "\n",
      "==============================================================================\n",
      "Finding good initial image pair\n",
      "==============================================================================\n",
      "\n",
      "  => No good initial image pair found.\n",
      "\n",
      "Elapsed time: 5.811 [minutes]\n",
      "test/church/images/00090.png\n",
      "test/church/images/00092.png\n",
      "test/church/images/00087.png\n",
      "test/church/images/00050.png\n",
      "test/church/images/00068.png\n",
      "test/church/images/00083.png\n",
      "test/church/images/00096.png\n",
      "test/church/images/00069.png\n",
      "test/church/images/00081.png\n",
      "test/church/images/00042.png\n",
      "test/church/images/00018.png\n",
      "test/church/images/00030.png\n",
      "test/church/images/00024.png\n",
      "test/church/images/00032.png\n",
      "test/church/images/00026.png\n",
      "test/church/images/00037.png\n",
      "test/church/images/00008.png\n",
      "test/church/images/00035.png\n",
      "test/church/images/00021.png\n",
      "test/church/images/00010.png\n",
      "test/church/images/00039.png\n",
      "test/church/images/00011.png\n",
      "test/church/images/00013.png\n",
      "test/church/images/00006.png\n",
      "test/church/images/00012.png\n",
      "test/church/images/00029.png\n",
      "test/church/images/00001.png\n",
      "test/church/images/00072.png\n",
      "test/church/images/00066.png\n",
      "test/church/images/00104.png\n",
      "test/church/images/00058.png\n",
      "test/church/images/00059.png\n",
      "test/church/images/00111.png\n",
      "test/church/images/00061.png\n",
      "test/church/images/00060.png\n",
      "test/church/images/00074.png\n",
      "test/church/images/00076.png\n",
      "test/church/images/00063.png\n"
     ]
    }
   ],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = True\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = 0\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = 1\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8000,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 50,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1680,\n",
    "        \"min_matches\": 100,\n",
    "    }\n",
    "    params_sgs = [params_sg3,]\n",
    "\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, CONFIG.CAMERA_MODEL, single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return\n",
    "\n",
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torchvision import transforms\n",
    "\n",
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, model, model2,\n",
    "                    device =  device):\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    model2 = model2.eval()\n",
    "    model2 = model2.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize(600, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.CenterCrop(600),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.4850, 0.4560, 0.4060], [0.2290, 0.2240, 0.2250]),])#create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #descf = model.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n",
    "            desc2 = model2.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #desc2f = model2.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n",
    "            #desc = desc+descf\n",
    "            #desc2 = desc2+desc2f\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc2 = desc2.view(1, -1)\n",
    "            desc_norm = torch.cat([desc, desc2], dim=-1)\n",
    "            desc_norm = F.normalize(desc_norm, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "def get_image_pairs_shortlist_nn(fnames,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              nneighbor=6,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less or num_imgs <= nneighbor :\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    model = timm.create_model('tf_efficientnet_b7',\n",
    "                             checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    \n",
    "    model.eval()\n",
    "    model2 = timm.create_model('tf_efficientnet_b6',\n",
    "                             checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b6/1/tf_efficientnet_b6_aa-80ba17e4.pth')\n",
    "    \n",
    "    model2.eval()\n",
    "    descs = get_global_desc(fnames, model, model2, device=device)\n",
    "    dm = torch.einsum('bi,ki->bk', descs, descs).detach().cpu()\n",
    "    print(f\"sim shape: {dm.shape}\")\n",
    "    \n",
    "    val, indices = torch.topk(dm, k=nneighbor, dim=1)\n",
    "\n",
    "    matching_list = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        for t in indices[st_idx][val[st_idx]>0.22]:\n",
    "            if t == st_idx:\n",
    "                continue\n",
    "            matching_list.append(tuple(sorted((st_idx, t.item()))))\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    del model, model2\n",
    "    torch.cuda.empty_cache()\n",
    "    return matching_list\n",
    "\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=True\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    \n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features, detection_threshold=detection_threshold, resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "        \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    dict_kpts_rot_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts_rot = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts_rot), -1).detach()\n",
    "            dict_kpts_rot_cuda[f\"{key}\"] = kpts_rot\n",
    "            \n",
    "            kpts = convert_coord(kpts_rot, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1_rot = dict_kpts_rot_cuda[key1]\n",
    "            kp2_rot = dict_kpts_rot_cuda[key2]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            \n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1_rot[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2_rot[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.superpoint import SuperPoint as SG_SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          process_resize, frame2tensor,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "from torch.nn import functional as torchF  # For resizing tensor\n",
    "\n",
    "# Preprocess\n",
    "def sg_read_image(image, device, resize):\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, [resize,])\n",
    "    \n",
    "    unit_shape = 8\n",
    "    w_new = w_new // unit_shape * unit_shape\n",
    "    h_new = h_new // unit_shape * unit_shape\n",
    "    \n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "\n",
    "    inp = frame2tensor(image, \"cpu\")\n",
    "    return image, inp, scales, (h, w)\n",
    "\n",
    "class SGDataset(Dataset):\n",
    "    def __init__(self, img_fnames, resize_to, device):\n",
    "        self.img_fnames = img_fnames\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.img_fnames[idx]\n",
    "        im = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        _, image, scale, ori_shape = sg_read_image(im, self.device, self.resize_to)\n",
    "        return image, torch.tensor([idx]), torch.tensor(ori_shape)\n",
    "\n",
    "def get_superglue_dataloader(img_fnames, resize_to, device, batch_size=1):\n",
    "    dataset = SGDataset(img_fnames, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def detect_superglue(\n",
    "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints, file_keypoints_crop,\n",
    "    resize_to=750, min_matches=15\n",
    "):    \n",
    "    t=time()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    dataloader = get_superglue_dataloader(img_fnames=img_fnames, resize_to=1680, device=device)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    superpoint = SG_SuperPoint(sg_config[\"superpoint\"]).eval().to(device)\n",
    "    dict_features_cuda = {}\n",
    "    dict_shapes = {}\n",
    "    dict_images = {}\n",
    "    #dict_fname_shapes = {}\n",
    "    for X in dataloader:\n",
    "        image, idx, ori_shape = X\n",
    "        image = image[0].to(device)\n",
    "        fname = img_fnames[idx]\n",
    "        #dict_fname_shapes[fname] = ori_shape\n",
    "        key = fname.split('/')[-1]\n",
    "        \n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            pred = superpoint({'image': image})\n",
    "            dict_features_cuda[key] = pred\n",
    "            dict_shapes[key] = ori_shape\n",
    "            dict_images[key] = image.half()\n",
    "    del superpoint\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    superglue = SuperGlue(sg_config[\"superglue\"]).eval().to(device)\n",
    "    weights = sg_config[\"superglue\"][\"weights\"]\n",
    "    cnt_pairs = 0\n",
    "    \n",
    "    mkpt = {}\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx, (fname1, fname2) in enumerate(zip(fnames1, fnames2)):\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            data = {\"image0\": dict_images[key1], \"image1\": dict_images[key2]}\n",
    "            data = {**data, **{k+'0': v for k, v in dict_features_cuda[key1].items()}}\n",
    "            data = {**data, **{k+'1': v for k, v in dict_features_cuda[key2].items()}}\n",
    "            for k in data:\n",
    "                if isinstance(data[k], (list, tuple)):\n",
    "                    data[k] = torch.stack(data[k])\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = {**data, **superglue(data)}\n",
    "                pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
    "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "\n",
    "            valid = matches > -1\n",
    "            mkpts1 = mkpts1[valid]\n",
    "            mkpts2 = mkpts2[matches[valid]]\n",
    "            mconf = conf[valid]\n",
    "\n",
    "            ori_shape_1 = dict_shapes[key1][0].numpy()\n",
    "            ori_shape_2 = dict_shapes[key2][0].numpy()\n",
    "            \n",
    "            # Scaling coords\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / dict_images[key1].shape[3]   # X\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / dict_images[key1].shape[2]   # Y\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / dict_images[key2].shape[3]   # X\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / dict_images[key2].shape[2]   # Y  \n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            if fname1 in mkpt:\n",
    "                mkpt[fname1] = np.concatenate([mkpt[fname1], mkpts1], axis=0).astype(np.float32)\n",
    "            else:\n",
    "                mkpt[fname1] = mkpts1\n",
    "            if fname2 in mkpt:\n",
    "                mkpt[fname2] = np.concatenate([mkpt[fname2], mkpts2], axis=0).astype(np.float32)\n",
    "            else:\n",
    "                mkpt[fname2] = mkpts2\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue/{resize_to}/{weights})')            \n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    \n",
    "       \n",
    "    \n",
    "    gc.collect()\n",
    "    del superglue\n",
    "    del dict_features_cuda\n",
    "    del dict_images\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t\n",
    "\n",
    "# Making kornia local features loading w/o internet\n",
    "class AffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "        detector_type='GFTT',\n",
    "    ):\n",
    "        config = {\n",
    "            # Extraction Parameters\n",
    "            \"nms_size\": 15,\n",
    "            \"pyramid_levels\": 4,\n",
    "            \"up_levels\": 1,\n",
    "            \"scale_factor_levels\": math.sqrt(2),\n",
    "            \"s_mult\": 22.0,\n",
    "        }\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        #detector = KF.KeyNetDetector(\n",
    "        #    False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        #).to(device)\n",
    "        if detector_type=='GFTT':\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.CornerGFTT(),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        elif detector_type=='DoG':\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.BlobDoGSingle(),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        else:\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.CornerHarris(0.04),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        #kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        #detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)\n",
    "\n",
    "# Making kornia local features loading w/o internet\n",
    "class KeyNetAffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "        \n",
    "    ):\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        detector = KF.KeyNetDetector(\n",
    "            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        ).to(device)\n",
    "        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)\n",
    "\n",
    "\n",
    "def detect_features(img_fnames,\n",
    "                    num_feats = 8000,\n",
    "                    upright = False,\n",
    "                    device=torch.device('cpu'),\n",
    "                    feature_dir = '.featureout',\n",
    "                    resize_small_edge_to = 1200,\n",
    "                   local_feature='Keynet'):\n",
    "\n",
    "    if local_feature == 'Keynet':\n",
    "        feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n",
    "    else:\n",
    "        feature = AffNetHardNet(num_feats, upright, device, detector_type=local_feature).to(device).eval()\n",
    "    print('local feature:', local_feature)\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in progress_bar(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                timg = load_torch_image(img_path, device=device)\n",
    "                H, W = timg.shape[2:]\n",
    "                if resize_small_edge_to is None:\n",
    "                    timg_resized = timg\n",
    "                else:\n",
    "                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n",
    "                    print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n",
    "                h, w = timg_resized.shape[2:]\n",
    "                \n",
    "                lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n",
    "                lafs[:,:,0,:] *= float(W) / float(w)\n",
    "                lafs[:,:,1,:] *= float(H) / float(h)\n",
    "                desc_dim = descs.shape[-1]\n",
    "                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n",
    "                f_laf[key] = lafs.detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return\n",
    "\n",
    "def match_features(img_fnames,\n",
    "                   index_pairs,\n",
    "                   file_keypoints,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=15, \n",
    "                   force_mutual = True,\n",
    "                   matching_alg='adalam'\n",
    "                  ):\n",
    "    assert matching_alg in ['smnn', 'adalam']\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n",
    "        h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(file_keypoints, mode='w') as f_match:\n",
    "\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "                    idx1, idx2 = pair_idx\n",
    "                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n",
    "                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n",
    "                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "                    kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "                    kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "                    if matching_alg == 'adalam':\n",
    "                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n",
    "                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n",
    "                        adalam_config = KF.adalam.get_adalam_default_config()\n",
    "                        #adalam_config['orientation_difference_threshold'] = None\n",
    "                        #adalam_config['scale_rate_threshold'] = None\n",
    "                        adalam_config['force_seed_mnn']= True\n",
    "                        adalam_config['search_expansion'] = 16\n",
    "                        adalam_config['ransac_iters'] = 128\n",
    "                        adalam_config['device'] = device\n",
    "                        dists, idxs = KF.match_adalam(desc1, desc2,\n",
    "                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n",
    "                                                      hw1=hw1, hw2=hw2,\n",
    "                                                      config=adalam_config) # Adalam also benefits from knowing image size\n",
    "                    else:\n",
    "                        dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n",
    "                    if len(idxs)  == 0:\n",
    "                        continue\n",
    "                    # Force mutual nearest neighbors\n",
    "                    if dists.mean().cpu().numpy() < 0.05:\n",
    "                        first_indices = get_unique_idxs(idxs[:,1])\n",
    "                        idxs = idxs[first_indices]\n",
    "                        dists = dists[first_indices]\n",
    "                    n_matches = len(idxs)\n",
    "                    kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "                    kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "                    if False:\n",
    "                        print (f'{key1}-{key2}: {n_matches} matches')\n",
    "                    group  = f_match.require_group(key1)\n",
    "                    if n_matches >= min_matches:\n",
    "                         group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return\n",
    "\n",
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    local_feature_model = 'GFTT' #Keynet GFTT DoG Harris\n",
    "    detect_features(img_fnames, \n",
    "                    20000,\n",
    "                    feature_dir=local_feature_model,\n",
    "                    upright=False,\n",
    "                    device=device,\n",
    "                    resize_small_edge_to=1200,\n",
    "                    local_feature=local_feature_model,#GFTT #DoG #Harris\n",
    "                               )\n",
    "    file_keypoints = f\"{feature_dir}/matches_{local_feature_model}.h5\"\n",
    "    match_features(img_fnames, index_pairs, file_keypoints, feature_dir=local_feature_model,device=device)\n",
    "    files_keypoints.append( file_keypoints )\n",
    "    \n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            file_keypoints_crop = f\"{feature_dir}/matches_superglue_{resize_to}pix_crop.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, file_keypoints_crop,\n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            #files_keypoints.append( file_keypoints_crop )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        \n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        \n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "def reconstruct_from_db(dataset, scene, feature_dir, img_dir, timings, image_paths):\n",
    "    scene_result = {}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    options = pycolmap.SiftMatchingOptions()\n",
    "    #options.confidence = 0.9999\n",
    "    #options.max_num_trials = 20000\n",
    "    pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    t=time() - t \n",
    "    timings['RANSAC'].append(t)\n",
    "    print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "    #mapper_options.num_threads = 1\n",
    "    mapper_options.min_model_size = 3\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    clear_output(wait=False)\n",
    "    t=time() - t\n",
    "    timings['Reconstruction'].append(t)\n",
    "    print(f'Reconstruction done in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    imgs_registered  = 0\n",
    "    best_idx = None\n",
    "    list_num_images = []            \n",
    "    print (\"Looking for the best reconstruction\")\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            print (idx1, rec.summary())\n",
    "            list_num_images.append( len(rec.images) )\n",
    "            if len(rec.images) > imgs_registered:\n",
    "                imgs_registered = len(rec.images)\n",
    "                best_idx = idx1\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if best_idx is not None:\n",
    "        print (maps[best_idx].summary())\n",
    "        for k, im in maps[best_idx].images.items():\n",
    "            key1 = f'test/{dataset}/images/{im.name}'\n",
    "            scene_result[key1] = {}\n",
    "            scene_result[key1][\"R\"] = deepcopy(im.rotmat())\n",
    "            scene_result[key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
    "\n",
    "    print(f'Registered: {dataset} / {scene} -> {len(scene_result)} images')\n",
    "    print(f'Total: {dataset} / {scene} -> {len(image_paths)} images')\n",
    "    print(timings)\n",
    "    return scene_result\n",
    "\n",
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')\n",
    "\n",
    "src = '/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# Get data from csv.\n",
    "data_dict = {}\n",
    "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            image, dataset, scene, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)\n",
    "            \n",
    "            if CONFIG.DRY_RUN:\n",
    "                if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "                    break\n",
    "                    \n",
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "\n",
    "out_results = {}\n",
    "timings = {\n",
    "    \"rotation_detection\" : [],\n",
    "    \"shortlisting\":[],\n",
    "   \"feature_detection\": [],\n",
    "   \"feature_matching\":[],\n",
    "   \"RANSAC\": [],\n",
    "   \"Reconstruction\": []\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    futures = defaultdict(dict)\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        if dataset not in out_results:\n",
    "            out_results[dataset] = {}\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(scene)\n",
    "            # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "            # You may want to run this on the training data in that case?\n",
    "            img_dir = f'{src}/test/{dataset}/images'\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            rots = [ 0 for fname in img_fnames ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print (f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            t=time()\n",
    "\n",
    "            index_pairs = get_image_pairs_shortlist_nn(img_fnames,\n",
    "                                                nneighbor=42,              \n",
    "                                                exhaustive_if_less = 42,)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################            \n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            futures[dataset][scene] = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene])\n",
    "                \n",
    "    #############################################################\n",
    "    # reconstruction results\n",
    "    #############################################################            \n",
    "    for dataset in datasets:\n",
    "        for scene in data_dict[dataset]:\n",
    "            # wait to complete COLMAP reconstruction\n",
    "            result = futures[dataset][scene].result()\n",
    "            if result is not None:\n",
    "                out_results[dataset][scene] = result   # get R and t from result\n",
    "    \n",
    "    create_submission(out_results, data_dict)\n",
    "    gc.collect()\n",
    "\n",
    "#!cat submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 2058261,
     "sourceId": 3414836,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3117886,
     "sourceId": 5373920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3364321,
     "sourceId": 5850511,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170475544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170565695,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174129945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175679956,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175684111,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2662,
     "sourceId": 3735,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 860.578212,
   "end_time": "2024-06-11T09:37:15.575892",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-11T09:22:54.997680",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
